utter_random_forest
- Random forests are an ensemble learning technique that builds off of  decision trees. Random forests involve creating multiple decision trees usingbootstrapped  datasetsof the original data and randomly selecting a subset of variables  at each step of the decision tree. The model then selects the mode of all  of the predictions of each decision tree. By relying on a “majority wins”  model, it reduces the risk of error from an individual tree. - For example,  if we created one decision tree, the third one, it would predict 0. But if  we relied on the mode of all 4 decision trees, the predicted value would be  1. This is the power of random forests. - Random forests offer several other  benefits including strong performance, can model non-linear boundaries, no  cross-validation needed, and gives feature importance. 